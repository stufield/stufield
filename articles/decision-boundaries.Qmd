---
title: "Decision boundaries: KNN vs Na&iuml;ve Bayes"
author: "Stu Field"
date: today
date-format: "D MMMM YYYY"
format:
  gfm:
    preview-mode: raw
    columns: 70
# output:
#   html_document:
#     code_folding: show
#     number_sections: yes
#     toc: yes
#     toc_float:
#       collapsed: no
editor_options:
  chunk_output_type: console
---



```{r}
#| label: setup
#| include: FALSE
knitr::opts_chunk$set(
  collapse = TRUE,
  comment  = "#>",
  fig.path = "figures/knn-bayes-"
)

library(helpr)
library(libml)

sim_data <- function(n = 200) {
  withr::local_seed(1001)
  # deterministic model
  # a = 3; b = 2.14; sd = 250
  # y_hat <- a * x^b
  # y <- rnorm(length(y.det), mean = y.det, sd = sd) # stochastic 'y'
  df   <- data.frame(x1 = rnorm(n, 45, 2), x2 = rnorm(n, 75, 2))
  df$y <- ifelse(df$x1 < 44 | df$x2 <= 74, "control", "disease") |> factor()
  df
}

sim_data2 <- function(n = 100) {
  withr::local_seed(999)
  df1 <- data.frame(x1 = rnorm(n, 46, 1.5), x2 = rnorm(n, 75, 1.5)) # control
  df2 <- data.frame(x1 = rnorm(n, 44, 1), x2 = rnorm(n, 78, 1))     # disease
  df  <- rbind(df1, df2)
  df$y <- factor(rep(c("control", "disease"), each = n))
  df
}


# Calculate KNN bivariate results
# method: passed to `dist()`
predict_bivariate_knn <- function(X, y, k, newdata, method = "minkowski") {
  if ( k < 2L ) {
    stop("Neighborhood (k) must be > 1: ", k, call. = FALSE)
  }
  if ( missing(newdata) ) {
    newdata <- X
  }
  X   <- data.matrix(X)
  ntr <- nrow(X)
  if ( length(y) != ntr ) {
    stop(
       sprintf("Length of class vector [y=%i] unequal to n training samples (n=%i)",
               length(y), ntr),
       call. = FALSE)
  }
  if ( ntr < k ) {
    warning(
      sprintf("Neighborhood (k=%i) exceeds training data (n=%i) ... resetting k=%i",
              k, ntr, ntr),
      call. = FALSE)
      k <- ntr
  }
  nte <- nrow(newdata)
  class_names <- names(table(y))
  neighbor_list <- lapply(seq_len(nte), function(.i) {
                          new_vals <- newdata[.i, ]
                          if ( length(new_vals) != 2L ) {
                            stop("Problem with new values ... length =",
                                 length(new_vals), call. = FALSE)
                          }
                          # kknn::kknn() uses Minkowski; class::knn() uses Euclid
                          dist_vec <- head(dist(rbind(new_vals, X),
                                                method = method), ntr) |>
                            setNames(seq_len(ntr)) |> sort()
                          head(dist_vec, k) |>  # get neighborhood (closest dist)
                            names() |>
                            as.integer()
  })
  neighbor_prop_disease <- vapply(neighbor_list, function(.x) {
    prop.table(table(y[.x]))[[class_names[2L]]] # prop disease in neighborhood
  }, double(1))
  classes <- ifelse(
    neighbor_prop_disease == 0.5,
    sample(class_names, 1, prob = prop.table(table(y))), # random tie-break
    ifelse(neighbor_prop_disease >= 0.5, class_names[2L], class_names[1L])
  )
  data.frame(class = classes, prob = neighbor_prop_disease)
}

# plot a bivariate decision boundary
plot_decision_boundary <- function(data, res = 50L,
                                   model_type = c("knn", "bayes"),
                                   k = 15L, line_col = "#00A499",
                                   lwd = 2, lty = 1, contours = 0.5) {
  y  <- data$y
  X  <- data[, c("x1", "x2")]
  x1 <- data$x1
  x2 <- data$x2
  x1grid <- seq(min(x1), max(x1), length = res)
  x2grid <- seq(min(x2), max(x2), length = res)
  grid <- expand.grid(x1 = x1grid, x2 = x2grid, KEEP.OUT.ATTRS = FALSE)
  model_type <- match.arg(model_type)
  if ( model_type == "bayes" ) {
    rm(k)
    model <- libml::fit_nb(y ~ ., data = data)
    prob_vec <- predict(model, grid, type = "raw")[, "disease"]
    title <- "Naive Bayes | disease (Pr>=0.5) space: "
  } else if ( model_type == "knn" ) {
    model <- predict_bivariate_knn(X, y, k, grid)
    prob_vec <- model$prob
    title <- sprintf("KNN (k=%i) | disease (Pr>=0.5) space: ", k)
  }
  prob_grid <- matrix(prob_vec, nrow = res)
  pos_space <- round(sum(prob_grid >= 0.5) / res^2, 3L)
  contour(x = x1grid, y = x2grid, z = prob_grid, levels = contours,
          lwd = lwd, lty = lty, labcex = 1, vfont = c("sans serif", "bold"),
          col = line_col, xlab = "Feature1", ylab = "Feature2",
          main = paste0(title, pos_space), axes = TRUE)
  col_d <- ggplot2::alpha("#840B55", 0.5)
  col_c <- ggplot2::alpha("steelblue", 0.5)
  points(grid, pch = "\u2022", cex = 0.75,
         col = ifelse(prob_vec >= 0.5, col_d, col_c))
  points(X, cex = 1.25, pch = 21, col = 1,
         bg = ifelse(y == "disease", col_d, col_c))
  invisible(data)
}
```


# Overview

It can be useful to visualize the decision boundaries of various models.
Here we compare 2 commonly used models:

* Na&iuml;ve Bayes
* *k*-nearest neighbors (KNN)


-----------------




## KNN vs Na&iuml;ve Bayes

Below are decision boundaries for 2 simulated data sets using *k*-nearest
neighbors and Na&iuml;ve Bayes models. In the first data set (upper 2 panels) the
true class boundary is simulated such that the disease (purple) Feature_1 \> 44
and Feature_2 \> 74, these data are simulated with an *unrealistic*
threshold to form the classes. The lower panels are simulated from bivariate
normal distributions, somewhat more realistic, and show the difference in the
boundary between the two methods.


```{r}
#| label: knn-vs-bayes
#| echo: FALSE
#| fig.width: 11
#| fig.height: 5
withr::with_par(
  list(mgp = c(2.00, 0.75, 0.00), mar = c(3, 4, 3, 1), mfrow = 1:2L), {
  plot_decision_boundary(sim_data(), k = 15L)
  plot_decision_boundary(sim_data(), model_type = "bayes")
})
withr::with_par(
  list(mgp = c(2.00, 0.75, 0.00), mar = c(3, 4, 3, 1), mfrow = 1:2L), {
  plot_decision_boundary(sim_data2(), k = 15L)
  plot_decision_boundary(sim_data2(), model_type = "bayes")
})
```


## Choosing *k* in KNN

```{r}
#| label: knn-k
#| fig.width: 11
#| fig.height: 10
withr::with_par(
  list(mgp = c(2.00, 0.75, 0.00), mar = c(3, 4, 3, 1), mfrow = c(3L, 3L)), {
  for ( i in 2:10L ) {
    plot_decision_boundary(sim_data(), k = i)
  }
})
```

----------------

### Code Reference

```{r}
#| label: code
# simulated data set 1
sim_data

# simulated data set 1
sim_data2

# predicting nearest neighbors from scratch
predict_bivariate_knn

# plotting routine for decision boundary
plot_decision_boundary
```
